{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cppapi\n",
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thomas/cpp-projects/low-level-deep-learning/examples\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('/home/thomas/cpp-projects/low-level-deep-learning')\n",
    "\n",
    "from python_tests.test_activation import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "data, target = make_regression(n_samples=1000, n_features=10, noise=2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 (1000, 32) B1 (1, 32) c++ M1 (1000, 32)\n",
      "c++ N1 (1000, 32) py N1 (1000, 32)\n",
      "True\n",
      "237.98251342773438 237.98255461583824\n",
      "The loss values after one step are approx equal: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Computational graph of the simple hard coded neural network we are testing\n",
    "#X-> |---------|          |-------- |                        |--------|          \n",
    "#    | v(X, W)|  -> M1 -> |A(M1 + B0)|  --> sigma(M1 + B0)-> |v(O1,W2)|  -> A(M2,B2) -> P ---> Lambda(P, Y) --> L                              \n",
    "#W-> |---------|          |-------- |                        |--------|        ^B2                      ^Y\n",
    "# here sigma represents a loss function\n",
    "#diabetes = load_diabetes()\n",
    "#targets = diabetes.target.astype(np.float32)\n",
    "#data = diabetes.data.astype(np.float32)\n",
    "targets_ = target.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "hidden_size = 32\n",
    "batch_size = 100\n",
    "num_features = data.shape[1]\n",
    "\n",
    "W1 = np.random.randn(num_features, hidden_size).astype(np.float32)\n",
    "W2 = np.random.randn(hidden_size, 1).astype(np.float32)\n",
    "B1 = np.random.randn(1, hidden_size).astype(np.float32)\n",
    "B2 = random.random()\n",
    "\n",
    "nn = cppapi.SimpleNeuralNetwork(batch_size, num_features, 1, hidden_size, W1, W2, B1, B2)\n",
    "\n",
    "loss = nn._forward_pass_one_step(cppapi.Activation.SIGMOID, cppapi.Loss.RMSE, data.astype(np.float32), targets_)\n",
    "\n",
    "# Now at this point we can access all of the objects inside the neural network class\n",
    "# so we need to make sure that a manual one step forward pass over a simple computation graph aligns\n",
    "rtol = 1e-3\n",
    "atol = 1e-3\n",
    "M1 = np.dot(data, W1)\n",
    "print(\"M1\", M1.shape, \"B1\", B1.shape, \"c++ M1\", nn.M1.shape)\n",
    "N1 = M1 + B1\n",
    "print(\"c++ N1\", nn.N1.shape, \"py N1\", N1.shape)\n",
    "print(np.allclose(N1, nn.N1, rtol, atol))\n",
    "O1 = sigmoid(N1)\n",
    "M2 = np.dot(O1, W2)\n",
    "P = M2 + B2\n",
    "pyloss = np.sqrt(np.mean(np.power(targets_ - P, 2)))\n",
    "print(loss, pyloss)\n",
    "print(\"The loss values after one step are approx equal:\", np.isclose(pyloss, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for one step of the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn._backward_pass(cppapi.Activation.SIGMOID, data.astype(np.float32), targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def deriv(func: Callable[[np.ndarray], np.ndarray],\n",
    "          input_: np.ndarray,\n",
    "          delta: float = 0.001) -> np.ndarray:\n",
    "    return (func(input_ + delta) - func(input_ - delta)) / (2. * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "(1000, 1)\n",
      "dLdM2 (1000, 1000)\n",
      "dLdP True\n",
      "c++ dLdP (1000, 1)\n",
      "(1000, 1) (1,)\n",
      "dLdB2 -7229.065436884959 c++ -7229.0654296875\n",
      "dM2dW2 (32, 1000) dPdM2 (1000, 1) dLdP (1000, 1)\n",
      "(32, 1) (32, 1)\n",
      "True\n",
      "(1, 32)\n",
      "N1 (1000, 32)\n",
      "dO1dN1 (1000, 32) dLdO1 (1000, 32)\n",
      "dO1dN1: True\n",
      "dLdO1: True\n",
      "python specific dLdN1 True\n",
      "dLdN1: True\n",
      "Max Abs diff dLdN1 7.208999281838047e-05\n",
      "python dLdN1 (1000, 32) [1, 32]\n",
      "c++ dLdB1 (1, 32)\n",
      "dLdB1 True\n",
      "[   74.36572416   -94.01144567 -1245.70562777 -2931.52306057\n",
      "  1288.37632207  -841.61262539  -150.87037323  2381.30506004\n",
      "  3549.35288238 -5486.58458957  -199.37186708  1152.5002679\n",
      " -4569.4836983   -764.71480892   112.97116454  -522.67493637\n",
      " -1762.78415432  1120.76075094  -235.35822516  -160.22797726\n",
      " -1233.48978973  3939.08062611   184.7840942     14.8061001\n",
      " -2317.9885899  -3036.68273995 -1227.38893772 -2923.50815121\n",
      "    88.21299336  3223.28021413 -1108.11527604  -254.97832133]\n",
      "[   74.36572416   -94.01144567 -1245.70562777 -2931.52306057\n",
      "  1288.37632207  -841.61262539  -150.87037323  2381.30506004\n",
      "  3549.35288238 -5486.58458957  -199.37186708  1152.5002679\n",
      " -4569.4836983   -764.71480892   112.97116454  -522.67493637\n",
      " -1762.78415432  1120.76075094  -235.35822516  -160.22797726\n",
      " -1233.48978973  3939.08062611   184.7840942     14.8061001\n",
      " -2317.9885899  -3036.68273995 -1227.38893772 -2923.50815121\n",
      "    88.21299336  3223.28021413 -1108.11527604  -254.97832133]\n"
     ]
    }
   ],
   "source": [
    "dLdP = -(targets_ - P)\n",
    "print(dLdP.shape)\n",
    "print(M2.shape)\n",
    "dPdM2 = np.ones_like(M2)\n",
    "dLdM2 = dLdP @ dPdM2.transpose()\n",
    "print(\"dLdM2\", dLdM2.shape)\n",
    "dPdB2 = np.ones([1, 1])\n",
    "dLdB2 = (dLdP * dPdB2).sum(axis=0)\n",
    "\n",
    "rtol = 1e-4\n",
    "atol = 1e-4\n",
    "print(\"dLdP\", np.allclose(nn.get_dLdP(), dLdP, rtol, atol))\n",
    "print(\"c++ dLdP\", nn.get_dLdP().shape)\n",
    "print(dLdP.shape, dLdB2.shape)\n",
    "print(\"dLdB2\", dLdB2.squeeze(), \"c++\", nn.dLdB2)\n",
    "\n",
    "dM2dW2 = np.transpose(O1, (1, 0))\n",
    "dLdW2 = dM2dW2 @ dLdP\n",
    "print(\"dM2dW2\", dM2dW2.shape, \"dPdM2\", dPdM2.shape, \"dLdP\", dLdP.shape)\n",
    "print(dLdW2.shape, nn.dLdW2.shape)\n",
    "print(np.allclose(dLdW2, nn.dLdW2, rtol, atol))\n",
    "\n",
    "dM2dO1 = np.transpose(W2, (1, 0))\n",
    "print(dM2dO1.shape)\n",
    "dLdO1 = dLdP @ dM2dO1\n",
    "print(\"N1\", N1.shape)\n",
    "# Using the finite differences method causes a numerical error between the matrices\n",
    "# and dO1dN1 across the FFI are not always the same\n",
    "#dO1dN1 = deriv(sigmoid, N1, 0.0001)\n",
    "dO1dN1 = sigmoid(N1) * (1.0 - sigmoid(N1))\n",
    "print(\"dO1dN1\", dO1dN1.shape, \"dLdO1\", dLdO1.shape)\n",
    "dLdN1 = dLdO1 * dO1dN1\n",
    "print(\"dO1dN1:\", np.allclose(dO1dN1, nn.get_dO1dN1(), rtol, atol))\n",
    "print(\"dLdO1:\", np.allclose(dLdO1, nn.get_dLdO1(), rtol, atol))\n",
    "# inputs to dLdN1 are dLdO1 and dO1dN1\n",
    "print(\"python specific dLdN1\", np.allclose(dLdN1, nn.get_dLdO1() * nn.get_dO1dN1(), rtol, atol))\n",
    "print(\"dLdN1:\", np.allclose(dLdN1, nn.get_dLdN1(), rtol, atol))\n",
    "print(\"Max Abs diff dLdN1\", np.max(np.abs(dLdN1 - nn.get_dLdN1())))\n",
    "print(\"python dLdN1\", dLdN1.shape, f\"[1, {hidden_size}]\")\n",
    "dLdB1 = (dLdN1 * np.ones([1, hidden_size])).sum(axis=0)\n",
    "print(\"c++ dLdB1\", nn.dLdB1.shape)\n",
    "print(\"dLdB1\", np.allclose(dLdB1, nn.dLdB1, rtol, atol))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cppdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
